# 机器人自动走迷宫

## 1. 项目介绍

### 1.1 项目内容

在本实验中，要求分别使用基础搜索算法和 Deep QLearning 算法，完成机器人自动走迷宫。

<img src="https://imgbed.momodel.cn/20200914145238.png" width="40%"/>

如上图所示，左上角的红色椭圆既是起点也是机器人的初始位置，右下角的绿色方块是出口。          
游戏规则为：从起点开始，通过错综复杂的迷宫，到达目标点(出口)。
        

+ 在任一位置可执行动作包括：向上走 `'u'`、向右走 `'r'`、向下走 `'d'`、向左走 `'l'`。

+ 执行不同的动作后，根据不同的情况会获得不同的奖励，具体而言，有以下几种情况。
    - 撞墙
    - 走到出口
    - 其余情况
    
+ 需要您分别实现**基于基础搜索算法**和 **Deep QLearning 算法**的机器人，使机器人自动走到迷宫的出口。

### 1.2 项目要求

+ 使用 Python 语言。
+ 使用基础搜索算法完成机器人走迷宫。
+ 使用 Deep QLearning 算法完成机器人走迷宫。
+ 算法部分需要自己实现，不能使用现成的包、工具或者接口。



## 2. 基础搜索算法解决

### 2.1 A* 算法

A* 算法是一种启发式搜索算法，结合了 **Dijkstra 算法**（基于路径成本）和 **贪心算法**（基于启发函数）来高效寻找最短路径。它通过维护一个优先队列（open_list）来探索可能的路径，并使用启发函数估算从当前位置到目标的成本。

A* 算法的核心公式为：

```txt
f(n) = g(n) + h(n)
```

- `g(n)`：从起点到当前节点 `n` 的实际路径成本。
- `h(n)`：从当前节点 `n` 到目标的估计成本（启发函数）。
- `f(n)`：总估计成本，用于优先队列排序。

### 2.2 实现思路

`my_search` 函数的目标是根据给定的迷宫对象 `maze`，从起点到终点找到一条最短路径，并返回路径上每一步的移动方向列表（例如 `['u', 'u', 'r']` 表示向上移动两次、向右移动一次）。如果无法到达终点，则返回空列表 `[]`。

迷宫对象 `maze` 提供了以下接口：
1. `maze.sense_robot()`：返回起点坐标 `(row, col)`。
2. `maze.destination`：返回终点坐标 `(row, col)`。
3. `maze.can_move_actions(pos)`：给定坐标 `pos`，返回从该位置可以移动的方向列表（如 `['u', 'r']`，表示可以向上或向右移动）

1. **方向定义**：
   ```python
   direction_offsets = {
       'u': (-1, 0),  # 向上：行坐标减1
       'r': (0, 1),   # 向右：列坐标加1
       'd': (1, 0),   # 向下：行坐标加1
       'l': (0, -1),  # 向左：列坐标减1
   }
   ```
   定义了四个移动方向（上、右、下、左）及其对应的坐标变化量。

2. **启发函数**：
   ```python
   def heuristic(pos, goal):
       return abs(pos[0] - goal[0]) + abs(pos[1] - goal[1])
   ```
   使用 **曼哈顿距离** 作为启发函数，计算当前位置 `pos` 到目标 `goal` 的估计距离。曼哈顿距离适用于网格迷宫（只允许上下左右移动），因为它是可接受的（admissible）且一致的（consistent），不会高估实际路径成本。

3. **初始化**：
   
   ```python
   start = maze.sense_robot()
   goal = maze.destination
   open_list = []
   heapq.heappush(open_list, (heuristic(start, goal), 0, start, []))
   visited = {}
   ```
   - 获取起点 `start` 和终点 `goal` 坐标。
   - 初始化优先队列 `open_list`，使用 `heapq` 实现。每个元素是一个元组 `(f, g, current_pos, path_so_far)`：
     - `f`：总估计成本（`g + h`）。
     - `g`：从起点到当前位置的实际路径成本。
     - `current_pos`：当前坐标。
     - `path_so_far`：到当前位置的路径（动作列表）。
   - 初始化 `visited` 字典，记录每个位置的最小 `g` 值，用于避免重复处理次优路径。
   
4. **主循环**：
   
   ```python
   while open_list:
       f, g, current_pos, path_so_far = heapq.heappop(open_list)
       if current_pos in visited and visited[current_pos] <= g:
           continue
       visited[current_pos] = g
       if current_pos == goal:
           return path_so_far
   ```
   - 从优先队列中取出 `f` 值最小的节点。
   - 如果当前节点已被访问且已有更低的 `g` 值，则跳过（优化，避免重复处理）。
   - 更新 `visited` 字典，记录当前节点的 `g` 值。
   - 如果当前节点是目标节点，直接返回路径 `path_so_far`。
   
5. **扩展节点**：
   
   ```python
   for action in maze.can_move_actions(current_pos):
       delta_row, delta_col = direction_offsets[action]
       next_pos = (current_pos[0] + delta_row, current_pos[1] + delta_col)
       new_g = g + 1
       new_f = new_g + heuristic(next_pos, goal)
       heapq.heappush(open_list, (new_f, new_g, next_pos, path_so_far + [action]))
   ```
   - 获取当前节点的可移动方向（通过 `maze.can_move_actions`）。
   - 对每个可移动方向：
     - 计算新坐标 `next_pos`。
     - 更新路径成本 `new_g = g + 1`（每移动一步成本为 1）。
     - 计算总估计成本 `new_f = new_g + heuristic(next_pos, goal)`。
     - 将新节点加入优先队列，路径为 `path_so_far + [action]`。
   
6. **无路径情况**：
   ```python
   return []
   ```
   如果优先队列为空（无法到达终点），返回空列表。



## 3. 通过Deep QLearning算法解决该问题

Q-Learning 是一个值迭代（Value Iteration）算法。   
与策略迭代（Policy Iteration）算法不同，值迭代算法会计算每个”状态“或是”状态-动作“的值（Value）或是效用（Utility），然后在执行动作的时候，会设法最大化这个值。   
因此，对每个状态值的准确估计，是值迭代算法的核心。    
通常会考虑**最大化动作的长期奖励**，即不仅考虑当前动作带来的奖励，还会考虑动作长远的奖励。

### 3.1 DQN介绍

![image-20250603141044948](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20250603141044948.png)

<img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20250603141110048.png" alt="image-20250603141110048" style="zoom: 50%;" />

### 3.2 算法特点

- **强化学习**：使用 Q-learning 或深度 Q 网络（DQN），通过神经网络（`self.eval_model`）预测状态-动作对的 Q 值。
- **经验回放**：`self.memory` 暗示使用了经验回放机制，存储状态-动作-奖励-下一状态的四元组。
- **奖励机制**：
  - 撞墙奖励（`10.0`）和终点奖励（`-70.0`）的正负号设计需要进一步确认，可能与最小化路径长度相关。
  - 默认奖励（`1.0`）鼓励移动，适用于稀疏奖励环境。
- **训练与测试分离**：
  - `train_update` 用于训练，可能结合探索策略（如 ε-贪心）。
  - `test_update` 用于测试，使用训练好的模型选择最优动作。

### 3.3 智能体设计（Robot类）

#### 3.3.1 继承结构

```
python


CopyEdit
class Robot(TorchRobot):
```

本类继承自 `MinDQNRobot`，该基类封装了 DQN 强化学习核心逻辑，如经验回放、动作选择、模型更新等。

我们在此基础上进行扩展，使训练流程更加清晰、逻辑更加严密，并便于后续维护和调试。

------

#### 3.3.2 奖励机制设计

```
pythonCopyEditself.maze.set_reward({
    "hit_wall": 10.0,
    "destination": -70.0,
    "default": 1.0,
})
```

本项目采取“负奖励驱动”机制，设计如下：

| 行为     | 奖励    | 意图                 |
| -------- | ------- | -------------------- |
| 撞墙     | `+10.0` | 惩罚行为（Q 值高）   |
| 到达终点 | `-70.0` | 强化目标（Q 值低）   |
| 正常移动 | `+1.0`  | 中性偏惩罚，限制乱走 |

DQN 中目标是**最小化 Q 值**，因此终点对应 Q 最小，撞墙为次之，其他行动 Q 值适中。

------

#### 3.3.3 模型训练流程

智能体训练过程如下：

```
pythonCopyEditself.memory.build_full_view(maze=self.maze)
self.lost_list = self._train_until_success()
```

- 首先使用 `build_full_view` 采集完整训练集（静态环境允许）；
- 每轮调用 `_learn()` 更新网络权重；
- 每轮训练结束后，进入 `_test_robot_success()` 方法测试是否能成功走到终点；
- 若成功，则停止训练并返回损失序列 `lost_list`。

该方法较为鲁棒，适用于离线训练。

------

#### 3.3.4 状态感知与动作执行

#### 训练模式：

```
pythonCopyEditdef train_update(self):
    state = self.sense_state()
    action = self._choose_action(state)
    reward = self.maze.move_robot(action)
```

此函数用于训练期间交互，选择行为并执行动作，获取奖励。

#### 测试模式：

```
pythonCopyEditdef test_update(self):
    state = self.sense_state()
    q_values = self.eval_model(state)
    action = argmin(q_values) → 最佳动作
    reward = maze.move_robot(action)
```

测试不更新网络，仅用于评估当前模型的推理能力。

------

#### 3.3.5 可读性优化与逻辑重构

为了提高代码的可读性与维护性，我们在继承原类基础上重构了以下几个方法：

| 原设计               | 新设计                           | 优点               |
| -------------------- | -------------------------------- | ------------------ |
| 初始化中混合训练过程 | 拆出 `_train_until_success()`    | 分离逻辑，结构清晰 |
| 训练成功条件散落     | `_test_robot_success()` 独立实现 | 重复利用，简洁明了 |
| 奖励设置硬编码       | `_configure_reward()` 单独实现   | 易于后期策略切换   |



### 4. 总结

本实验通过实现基于 **A* 算法** 和 **Deep Q-Learning (DQN)** 的机器人自动走迷宫问题，展示了两种不同方法在解决路径规划问题中的应用。

1. **A* 算法**：
   - 采用启发式搜索，通过结合实际路径成本（`g(n)`）和曼哈顿距离启发函数（`h(n)`）实现高效的最短路径搜索。
   - 利用优先队列（`heapq`）和访问记录（`visited`）优化搜索过程，确保在网格迷宫中快速找到从起点到终点的最优路径。
   - 实现简单、计算效率高，适合静态、确定性环境的路径规划问题。

2. **Deep Q-Learning (DQN)**：
   - 使用强化学习框架，通过神经网络（`eval_model`）估计状态-动作对的 Q 值，结合经验回放（`memory`）和 ε-贪心策略进行训练。
   - 通过迭代学习优化策略，适应动态或复杂环境，能够处理奖励稀疏的迷宫问题。
   - 实现包括训练（`train`）、训练更新（`train_update`）和测试更新（`test_update`），通过最小化 Q 值选择最优动作，成功引导机器人到达终点。

